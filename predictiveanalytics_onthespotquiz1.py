# -*- coding: utf-8 -*-
"""PredictiveAnalytics OnTheSpotQuiz1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I-g4QH4VubAMt9anzwcBzfO1rlRy0xrS
"""

import pandas as pd
import numpy as np
import random
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from xgboost import XGBClassifier

np.random.seed(42)
data = pd.read_csv('/content/drive/MyDrive/Predictive analytics Practice/DSAI-LVA-DATASET for Quiz.csv')
data.head()
data = data.drop('ParentEducation',axis = 1)
selected_values = ['Masters','Bachelors','High School','School','Not Educated']
random_values = [random.choice(selected_values) for _ in range(len(data))]
data['ParentEducation'] = random_values
data.head()
threshold = 50
for index, row in data.iterrows():
    if row['PreviousTestScore'] >= threshold and row['PreviousTestScore'] < 80:
        data.at[index, 'Pass'] = 'Pass with low score'
    elif row['PreviousTestScore'] >= 80:
        data.at[index, 'Pass'] = 'Pass with high score'
    else:
        data.at[index, 'Pass'] = 'Fail'
data.head()

data.info()

encoder = LabelEncoder()
data['ParentEducation']=encoder.fit_transform(data['ParentEducation'])
data['Pass']=encoder.fit_transform(data['Pass'])
data.head()

encoder = LabelEncoder()
data['ParentEducation']=encoder.fit_transform(data['ParentEducation'])
data['Pass']=encoder.fit_transform(data['Pass'])
data.head()

correlation_matrix = data.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='magma')
plt.title('Heatmap of Correlation Matrix')
plt.show()

for i in data.columns:
  sns.histplot(data[i], kde=True)
  plt.title(f"Histplot of {i}")
  plt.xlabel(i)
  plt.ylabel('Frequency')
  plt.show()

sns.countplot(x='Pass', data=data)
plt.title('Countplot of Target Variable')
plt.xlabel('Pass')
plt.ylabel('Count')
plt.show()

split_ratio = 0.8
split_index = int(len(data) * split_ratio)

# Split the DataFrame
data1 = data.iloc[:split_index]
data2 = data.iloc[split_index:]

# Write data to separate files
data1.to_csv('Train_data.csv', index=False)
data2.to_csv('Test_data.csv', index=False)

train_data = pd.read_csv('/content/Train.csv')
test_data = pd.read_csv('/content/Test.csv')

X_train, y_train = train_data.drop('Pass', axis=1), train_data['Pass']
X_test, y_test = test_data.drop('Pass', axis=1), test_data['Pass']

model1 = LogisticRegression()
model1.fit(X_train, y_train)

model2 = RandomForestClassifier()
model2.fit(X_train, y_train)
model3 = XGBClassifier()
model3.fit(X_train, y_train)

model1_pred = model1.predict(X_test)
model2_pred = model2.predict(X_test)
model3_pred = model3.predict(X_test)

knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)
knn_pred = knn.predict(X_test)

acc_score_rf = accuracy_score(y_test,model2_pred)
print("Accuracy score of random forest: ",acc_score_rf)
acc_score = accuracy_score(y_test,model1_pred)
print("Accuracy score of Logistic Regression: ",acc_score)
acc_score_knn = accuracy_score(y_test,knn_pred)
print("Accuracy score of knn: ",acc_score_knn)
acc_score_XGB = accuracy_score(y_test,model3_pred)
print("Accuracy score of knn: ",acc_score_XGB)

data3 = ['Log_Reg','Rand_Forest','KNN','XGBoost']
sns.barplot([1.0,1.0,0.975,1.0])
plt.xlabel('Model')
plt.ylabel('Accuracy')
plt.show()